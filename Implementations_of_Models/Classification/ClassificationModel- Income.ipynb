{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I shall implement a classification model for the dataset that I found on this website:\n",
    "https://archive.ics.uci.edu/ml/datasets/adult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fnlwgt</th>\n",
       "      <th>Education-num</th>\n",
       "      <th>Capital-gain</th>\n",
       "      <th>Capital-loss</th>\n",
       "      <th>Hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32561.000000</td>\n",
       "      <td>3.256100e+04</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.581647</td>\n",
       "      <td>1.897784e+05</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>40.437456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.640433</td>\n",
       "      <td>1.055500e+05</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>12.347429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.178270e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.783560e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.370510e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Age        Fnlwgt  Education-num  Capital-gain  Capital-loss  \\\n",
       "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
       "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
       "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
       "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
       "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
       "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
       "\n",
       "       Hours-per-week  \n",
       "count    32561.000000  \n",
       "mean        40.437456  \n",
       "std         12.347429  \n",
       "min          1.000000  \n",
       "25%         40.000000  \n",
       "50%         40.000000  \n",
       "75%         45.000000  \n",
       "max         99.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./adult.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Workclass', 'Fnlwgt', 'Education', 'Education-num',\n",
      "       'Marital-status', 'Occupation', 'Relationship', 'Race', 'Sex',\n",
      "       'Capital-gain', 'Capital-loss', 'Hours-per-week', 'Native-Country',\n",
      "       'Income'],\n",
      "      dtype='object')\n",
      "(32561, 15)\n",
      "Age                int64\n",
      "Workclass         object\n",
      "Fnlwgt             int64\n",
      "Education         object\n",
      "Education-num      int64\n",
      "Marital-status    object\n",
      "Occupation        object\n",
      "Relationship      object\n",
      "Race              object\n",
      "Sex               object\n",
      "Capital-gain       int64\n",
      "Capital-loss       int64\n",
      "Hours-per-week     int64\n",
      "Native-Country    object\n",
      "Income            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df.shape)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The order of columns is an inconvenience to the way I will train the model.\n",
    "#I wish to have all columns that have a string as input (eg, education) to be at one end of the feature array and numbers at the other end\n",
    "#Also, Fnlwgt column at one end will also be convenient\n",
    "\n",
    "def SwapColumns(df, c1, c2):\n",
    "    '''\n",
    "    Args:\n",
    "    df (Dataframe) : the input data frame\n",
    "    c1, c2 (int) : The indices of the columns to be interchanged\n",
    "    \n",
    "    Returns:\n",
    "    df (Dataframe) : the updated Dataframe\n",
    "    '''\n",
    "\n",
    "    col_list = list(df.columns)\n",
    "    col1 = col_list[c1]\n",
    "    col2 = col_list[c2]\n",
    "    col_list[c1], col_list[c2] = col_list[c2], col_list[c1]\n",
    "    df[col1], df[col2] = df[col2], df[col1] #swapping the content of the columns\n",
    "    df.columns = col_list #swapping the column headers\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Sex          Workclass  Native-Country    Education    Race  \\\n",
      "0         Male          State-gov   United-States    Bachelors   White   \n",
      "1         Male   Self-emp-not-inc   United-States    Bachelors   White   \n",
      "2         Male            Private   United-States      HS-grad   White   \n",
      "3         Male            Private   United-States         11th   Black   \n",
      "4       Female            Private            Cuba    Bachelors   Black   \n",
      "...        ...                ...             ...          ...     ...   \n",
      "32556   Female            Private   United-States   Assoc-acdm   White   \n",
      "32557     Male            Private   United-States      HS-grad   White   \n",
      "32558   Female            Private   United-States      HS-grad   White   \n",
      "32559     Male            Private   United-States      HS-grad   White   \n",
      "32560   Female       Self-emp-inc   United-States      HS-grad   White   \n",
      "\n",
      "            Marital-status          Occupation    Relationship  Education-num  \\\n",
      "0            Never-married        Adm-clerical   Not-in-family             13   \n",
      "1       Married-civ-spouse     Exec-managerial         Husband             13   \n",
      "2                 Divorced   Handlers-cleaners   Not-in-family              9   \n",
      "3       Married-civ-spouse   Handlers-cleaners         Husband              7   \n",
      "4       Married-civ-spouse      Prof-specialty            Wife             13   \n",
      "...                    ...                 ...             ...            ...   \n",
      "32556   Married-civ-spouse        Tech-support            Wife             12   \n",
      "32557   Married-civ-spouse   Machine-op-inspct         Husband              9   \n",
      "32558              Widowed        Adm-clerical       Unmarried              9   \n",
      "32559        Never-married        Adm-clerical       Own-child              9   \n",
      "32560   Married-civ-spouse     Exec-managerial            Wife              9   \n",
      "\n",
      "       Age  Capital-gain  Capital-loss  Hours-per-week  Fnlwgt  Income  \n",
      "0       39          2174             0              40   77516   <=50K  \n",
      "1       50             0             0              13   83311   <=50K  \n",
      "2       38             0             0              40  215646   <=50K  \n",
      "3       53             0             0              40  234721   <=50K  \n",
      "4       28             0             0              40  338409   <=50K  \n",
      "...    ...           ...           ...             ...     ...     ...  \n",
      "32556   27             0             0              38  257302   <=50K  \n",
      "32557   40             0             0              40  154374    >50K  \n",
      "32558   58             0             0              40  151910   <=50K  \n",
      "32559   22             0             0              20  201490   <=50K  \n",
      "32560   52         15024             0              40  287927    >50K  \n",
      "\n",
      "[32561 rows x 15 columns]\n",
      "Index(['Sex', 'Workclass', 'Native-Country', 'Education', 'Race',\n",
      "       'Marital-status', 'Occupation', 'Relationship', 'Education-num', 'Age',\n",
      "       'Capital-gain', 'Capital-loss', 'Hours-per-week', 'Fnlwgt', 'Income'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = SwapColumns(df, 2, 13) #putting Fnlwgt at the 2nd last position\n",
    "df = SwapColumns(df, 0, 9) #swapping Age with Sex (features with input data type int at the end)\n",
    "df = SwapColumns(df, 4, 8) #swapping Education-num with Race\n",
    "print(df)\n",
    "print(df.columns)\n",
    "\n",
    "#Now, all columns from indices 0-7 have string inputs, 8-12 have int inputs, 13 has the weights and 14 has the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We treat >50K as the target 1 and <=50K as the target 0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(df['Income'] == ' >50K').astype(int)[:28000]\n",
    "print(y_train)\n",
    "'''We treat >50K as the target 1 and <=50K as the target 0'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' Male' ' State-gov' ' United-States' ... 0 40 77516]\n",
      " [' Male' ' Private' ' United-States' ... 0 40 234721]\n",
      " [' Female' ' Private' ' Cuba' ... 0 40 338409]\n",
      " ...\n",
      " [' Female' ' Private' ' United-States' ... 0 40 151910]\n",
      " [' Male' ' Private' ' United-States' ... 0 20 201490]\n",
      " [' Female' ' Self-emp-inc' ' United-States' ... 0 40 287927]]\n",
      "(30159, 14)\n"
     ]
    }
   ],
   "source": [
    "features_data = np.array(df)[:, :14]\n",
    "features_data = np.delete(features_data, np.where(features_data == ' ?'), axis=0)\n",
    "print(features_data)\n",
    "print(features_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77516 234721 338409 ... 140558 303455 76855]\n",
      "(28000, 13)\n",
      "[[' Male' ' State-gov' ' United-States' ... 2174 0 40]\n",
      " [' Male' ' Private' ' United-States' ... 0 0 40]\n",
      " [' Female' ' Private' ' Cuba' ... 0 0 40]\n",
      " ...\n",
      " [' Male' ' Self-emp-not-inc' ' United-States' ... 0 0 40]\n",
      " [' Female' ' Local-gov' ' United-States' ... 4787 0 60]\n",
      " [' Female' ' Self-emp-not-inc' ' United-States' ... 0 0 40]]\n"
     ]
    }
   ],
   "source": [
    "#28000 examples are used to train and remaining ~2k examples will be used to test the model\n",
    "\n",
    "weight_of_data = features_data[:28000, 13]\n",
    "print(weight_of_data)\n",
    "x_train = features_data[:28000, :13]\n",
    "print(x_train.shape)\n",
    "print(x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now a challenge that comes to my mind is how to deal with the columns that have their feature data of object data type. \n",
    "A solution that comes to my mind is that I can sort a particular feature (example, education) in increasing order of probability of the income being >50K. Post this, I will simply assign them numbers in that order.\n",
    "For eg, if we find that people with 11th education are amongst the fewest to have income >50K, followed by HS-Grad followed by Bachelors followed by Masters, we will assign the numbers\n",
    "\n",
    "0 to 11th Education\n",
    "\n",
    "1 to HS-Grad\n",
    "\n",
    "2 to Bachelors\n",
    "\n",
    "3 to Masters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range (8): # 0-7 have object data type for the input feature\n",
    "    input_labels = np.array(np.unique(x_train[:, i]))\n",
    "    n = input_labels.shape[0] #the number of possible values\n",
    "    df_temp = pd.DataFrame(np.zeros(3 * n).reshape(3,n), columns=input_labels)\n",
    "    # zeros_array = np.zeros(2 * n)\n",
    "    # temp = np.append(input_values, zeros_array)\n",
    "    # input_values_vs_freq = temp.reshape(3, n)\n",
    "    # find_where = (x_train == input_labels[0] and y_train == 1).all()\n",
    "    # print(find_where)\n",
    "    for j in range(x_train.shape[0]):\n",
    "        df_temp[x_train[j, i]][0] += 1\n",
    "        if(y_train[j] == 1):\n",
    "            df_temp[x_train[j, i]][1] += 1\n",
    "\n",
    "    for j in range(input_labels.shape[0]):\n",
    "        df_temp[input_labels[j]][2] = (df_temp[input_labels[j]][1] / df_temp[input_labels[j]][0]) * 100\n",
    "    print(df_temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''I'm discarding the above code and replacing it with a better DataFrame design to easily implement sorting'''\n",
    "\n",
    "Mapping_text_to_numbers = pd.DataFrame()\n",
    "\n",
    "for i in range (8): # 0-7 have object data type for the input feature\n",
    "    input_labels = np.array(np.unique(x_train[:, i]))\n",
    "    n = input_labels.shape[0] #the number of possible values\n",
    "    features_as_columns = input_labels.reshape(n, 1)\n",
    "    temp = np.zeros(3 * n)\n",
    "    temp = np.reshape(temp, (n, 3))\n",
    "    feature_reshaped = np.append(features_as_columns, temp, axis=1)\n",
    "    \n",
    "    df_temp = pd.DataFrame(feature_reshaped, columns= [\"Type\", \"Positive cases\", \"Total Cases\", \"Percentage Positive\"])\n",
    "\n",
    "    for j in range(x_train.shape[0]):\n",
    "        idx = np.where(input_labels == x_train[j, i])[0][0]\n",
    "        df_temp['Total Cases'][idx] += 1\n",
    "        if(y_train[j] == 1):\n",
    "            df_temp['Positive cases'][idx] += 1\n",
    "\n",
    "    for j in range(input_labels.shape[0]):\n",
    "        df_temp['Percentage Positive'][j] = (df_temp['Positive cases'][j] / df_temp['Total Cases'][j]) * 100\n",
    "\n",
    "    '''We now have our DataFrame. We can sort it and then conditionally replace the values of our training set with a value of our choice\n",
    "    We will also store the mapping for further testing of data'''\n",
    "\n",
    "    df_temp = df_temp.sort_values(by=[\"Percentage Positive\"])\n",
    "    Mapping_text_to_numbers = pd.concat([Mapping_text_to_numbers, \n",
    "                            df_temp[ ['Type', 'Percentage Positive'] ] \n",
    "                            ]\n",
    "                            )\n",
    "\n",
    "#Here I have 2 ideas : replace the value of x with % positive value or with j. % values makes more sense\n",
    "    for j in range(input_labels.shape[0]):\n",
    "        x_train[(x_train[:, i] == input_labels[j]), i] = df_temp['Percentage Positive'][j]\n",
    "        # x_train[ (x_train[:, i] == input_labels[j]) ] = df_temp['Percentage Positive'][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24.232804232804234 24.64013547840813 23.885624755189973 ... 2174 0 40]\n",
      " [24.232804232804234 23.499806426635693 23.885624755189973 ... 0 0 40]\n",
      " [23.384615384615383 23.499806426635693 23.809523809523807 ... 0 0 40]\n",
      " ...\n",
      " [24.232804232804234 24.65811965811966 23.885624755189973 ... 0 0 40]\n",
      " [23.384615384615383 26.461377870563673 23.885624755189973 ... 4787 0 60]\n",
      " [23.384615384615383 24.65811965811966 23.885624755189973 ... 0 0 40]]\n",
      "               Type Percentage Positive\n",
      "0            Female           23.384615\n",
      "1              Male           24.232804\n",
      "6       Without-pay           15.384615\n",
      "2           Private           23.499806\n",
      "0       Federal-gov           23.522727\n",
      "..              ...                 ...\n",
      "2    Other-relative           23.216445\n",
      "1     Not-in-family           23.926979\n",
      "0           Husband           24.023556\n",
      "3         Own-child           24.201681\n",
      "4         Unmarried           24.296247\n",
      "\n",
      "[98 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(Mapping_text_to_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(x_train.shape[1]):\n",
    "#     input_labels = np.array(np.unique(x_train[:,i]))\n",
    "#     freq = np.array([])\n",
    "#     for j in range(input_labels.shape[0]):\n",
    "#         freq = np.append(freq, np.sum(y_train[ x_train[:,i] == input_labels[j]]))\n",
    "#     plt.plot(input_labels, freq, marker='x', c='g')\n",
    "#     plt.xlabel(df.columns[i])\n",
    "#     plt.ylabel('Number of houses with income >50K')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(z):\n",
    "    if(z >= 30):\n",
    "        return 0.999988888888\n",
    "    elif(z <= -30):\n",
    "        return 0.000011111111\n",
    "    else:\n",
    "        f_wb = 1 / (1 + math.exp(-z))\n",
    "        return f_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def ComputeCost(X, y, weights, w, b):\n",
    "    '''\n",
    "    Args:\n",
    "    X (ndarray (m,n)) : The array containing the feature data for all training examples\n",
    "    y (ndarray (m,)) : The target values in the training data\n",
    "    w (ndarray (n,) : The parameter of Logistic Regression Model\n",
    "    b (scalar) : The parameter of Logistic Regression Model\n",
    "    g (function) : The sigmoid function\n",
    "    \n",
    "    Returns:\n",
    "    J (scalar) : Cost function for given parameters w and b\n",
    "    '''\n",
    "\n",
    "    m, n = X.shape\n",
    "    J = 0\n",
    "    for i in range(m):\n",
    "        z_i = (np.dot(w, X[i]) + b)\n",
    "        f_wb = sigmoid(z_i)\n",
    "        if(y[i]):\n",
    "            J += - (math.log(f_wb) * weights[i])\n",
    "        else:\n",
    "            J += - (math.log(1 - f_wb) * weights[i])\n",
    "    J /= (np.sum(weights))\n",
    "\n",
    "\n",
    "    '''\n",
    "    z = np.dot(X, w) + b\n",
    "    f_wb = 1 / (1 + np.exp(-z))\n",
    "    J = np.sum( - (np.dot(y_train, np.log(f_wb)) + np.dot((1 - y_train), np.log(1 - f_wb))\n",
    "    '''\n",
    "\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.418889759060642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Verifying that the ComputeCost() function is working correctly'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, n = x_train.shape\n",
    "w = np.array([0.5, 0.5, -0.5, 0.5, -0.5, 0.5, 0.5, 0.5, -0.5, 0.5, 0.5, -0.5, 0.5])\n",
    "b = 0\n",
    "\n",
    "print(ComputeCost(x_train, y_train, weight_of_data, w, b))\n",
    "'''Verifying that the ComputeCost() function is working correctly'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradient(X, y, weights, w, b):\n",
    "\n",
    "    ''''\n",
    "    Args:\n",
    "    X (ndarray (m,n)) : The array containing the feature data for all training examples\n",
    "    y (ndarray(m, )) : The target values in the training data\n",
    "    w (ndarray (n,) : The parameter of Logistic Regression Model\n",
    "    b (scalar) : The parameter of Logistic Regression Model\n",
    "    \n",
    "    Returns:\n",
    "    dj_dw (ndarray (n,)) : The gradient wrt w\n",
    "    b (scalar) : The gradient wrt b\n",
    "    '''\n",
    "\n",
    "    m, n = X.shape\n",
    "    dJ_dw = np.zeros(n)\n",
    "    f_wb = np.zeros(n)\n",
    "    dJ_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(w, X[i]) + b\n",
    "        f_wb = sigmoid(z_i)\n",
    "        dJ_dw = dJ_dw + ((f_wb - y[i]) * weights[i]) * X[i]\n",
    "        dJ_db += ((f_wb - y[i]) * weights[i])\n",
    "\n",
    "    a = np.sum(weights)\n",
    "    dJ_dw /= a\n",
    "    dJ_db /= a\n",
    "\n",
    "    return dJ_dw, dJ_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([6.251495820958986, 6.237575425788721, 6.23320680273025,\n",
      "       6.228052112581727, 6.248802713575994, 6.249171155314861,\n",
      "       6.236330353037311, 6.2506084324754045, 2.6083267119890046,\n",
      "       9.899220300366267, 329.46273791397056, 21.335317719879107,\n",
      "       10.669333850429217], dtype=object), 0.26095412828706205)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Verifying that the ComputeGradient() function is working correctly'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, n = x_train.shape\n",
    "w = np.zeros(n)\n",
    "b = 0\n",
    "\n",
    "print(ComputeGradient(x_train, y_train, weight_of_data, w, b))\n",
    "'''Verifying that the ComputeGradient() function is working correctly'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent(X, y, weights, w_in, b_in, alpha = 1.0e-3, epochs = 1000):\n",
    "    '''\n",
    "    Args:\n",
    "    X (ndarray (m,n)) : The array containing the feature data for all training examples\n",
    "    y (ndarray(m, )) : The target values in the training data\n",
    "    w_in (ndarray (n,) : The initial parameter of Logistic Regression Model\n",
    "    b_in (scalar) : The initial parameter of Logistic Regression Model\n",
    "    alpha (scalar) : learn rate of the algorithm\n",
    "    epochs (scalar) : Number of iterations to train the algorithm\n",
    "    \n",
    "    Returns: \n",
    "    w ('''\n",
    "    \n",
    "    J_record = []\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(epochs):\n",
    "        if(i % 3 == 0):\n",
    "            print(f'Iteration number {i}')\n",
    "        dJ_dw, dJ_db = ComputeGradient(X, y, weights, w, b)\n",
    "        w = w - dJ_dw * alpha\n",
    "        b = b - dJ_db * alpha\n",
    "\n",
    "        if(i % (epochs / 200) == 0):\n",
    "            J_record.append(ComputeCost(X, y, weights, w, b))\n",
    "            print(f'The cost is {J_record[-1]}')\n",
    "    \n",
    "    return w, b, J_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictOutput(test, w, b):\n",
    "    '''\n",
    "    Args:\n",
    "    test (ndarray(m,n)) : Test data with m training examples\n",
    "    w (ndarray(n,) : The final parameter of the model\n",
    "    b (scalar) : The final parameter of the model\n",
    "    \n",
    "    Returns :\n",
    "    predicted_outputs(ndarray(m,)) : The target predicted by the model\n",
    "    '''\n",
    "\n",
    "    m = test.shape[0]\n",
    "    predicted_outputs = []\n",
    "    \n",
    "    for i in range(m):\n",
    "        z = np.dot(test[i], w) + b\n",
    "        f_wb = sigmoid(z)\n",
    "        # print(f'z is {z} and f_wb is {f_wb}')\n",
    "        if(f_wb >= 0.5):\n",
    "            predicted_outputs.append(1)\n",
    "        else:\n",
    "            predicted_outputs.append(0)\n",
    "        # predicted_outputs.append(f_wb)\n",
    "    \n",
    "    return predicted_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_to_Req_Format(x_test, mapping):\n",
    "    m, n = x_test.shape\n",
    "    for i in range(8):\n",
    "        x_test[:, i] = mapping['Percentage Positive'].where(df['Type'] == x_test[:, i])\n",
    "    \n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before gradient descent, cost is 0.6931471805599475\n",
      "Iteration number 0\n",
      "The cost is 0.6923622656103833\n",
      "The cost is 0.6888159407903447\n",
      "The cost is 0.6872895825725073\n",
      "Iteration number 3\n",
      "The cost is 0.6866429520904433\n",
      "The cost is 0.6862462562397115\n",
      "The cost is 0.6858970325980477\n",
      "Iteration number 6\n",
      "The cost is 0.6855577062277948\n",
      "The cost is 0.6852242327606776\n",
      "The cost is 0.6848961667345443\n",
      "Iteration number 9\n",
      "The cost is 0.6845733190755529\n",
      "The cost is 0.6842555208775726\n",
      "The cost is 0.683942609442888\n",
      "Iteration number 12\n",
      "The cost is 0.68363442739252\n",
      "The cost is 0.6833308224975732\n",
      "The cost is 0.6830316475480936\n",
      "Iteration number 15\n",
      "The cost is 0.6827367602227149\n",
      "The cost is 0.6824460229576871\n",
      "The cost is 0.6821593028156953\n",
      "Iteration number 18\n",
      "The cost is 0.6818764713548914\n",
      "The cost is 0.6815974044986467\n",
      "The cost is 0.6813219824062287\n",
      "Iteration number 21\n",
      "The cost is 0.6810500893448421\n",
      "The cost is 0.6807816135633376\n",
      "The cost is 0.6805164471677062\n",
      "Iteration number 24\n",
      "The cost is 0.6802544859986802\n",
      "The cost is 0.6799956295115991\n",
      "The cost is 0.6797397806585703\n",
      "Iteration number 27\n",
      "The cost is 0.6794868457733304\n",
      "The cost is 0.6792367344585144\n",
      "The cost is 0.6789893594757849\n",
      "Iteration number 30\n",
      "The cost is 0.6787446366386087\n",
      "The cost is 0.6785024847079235\n",
      "The cost is 0.6782628252905305\n",
      "Iteration number 33\n",
      "The cost is 0.6780255827404046\n",
      "The cost is 0.6777906840628362\n",
      "The cost is 0.6775580588213242\n",
      "Iteration number 36\n",
      "The cost is 0.6773276390473741\n",
      "The cost is 0.6770993591530713\n",
      "The cost is 0.6768731558463381\n",
      "Iteration number 39\n",
      "The cost is 0.6766489680489752\n",
      "The cost is 0.6764267368173116\n",
      "The cost is 0.6762064052655686\n",
      "Iteration number 42\n",
      "The cost is 0.6759879184916426\n",
      "The cost is 0.6757712235055492\n",
      "The cost is 0.675556269160186\n",
      "Iteration number 45\n",
      "The cost is 0.6753430060846132\n",
      "The cost is 0.6751313866195828\n",
      "The cost is 0.6749213647553276\n",
      "Iteration number 48\n",
      "The cost is 0.6747128960715927\n",
      "The cost is 0.674505937679846\n",
      "[-0.00011667404828322112 -0.00011635857005630834 -0.00011633508789754102\n",
      " -0.00011607447095893542 -0.00011665940716819572 -0.0001166800261243157\n",
      " -0.00011635962705926388 -0.00011669892302722942 -4.823915588580407e-05\n",
      " -0.00018315986526736008 -4.47671691987529e-05 -0.0002879091120731969\n",
      " -0.00019784957013010333]\n",
      "-4.8716061694161345e-06\n"
     ]
    }
   ],
   "source": [
    "m, n = x_train.shape\n",
    "\n",
    "w_initial = np.zeros(n)\n",
    "b_initial = 0\n",
    "\n",
    "print(f'Before gradient descent, cost is {ComputeCost(x_train, y_train, weight_of_data, w_initial, b_initial)}')\n",
    "\n",
    "w_final, b_final, J_history = GradientDescent(x_train, y_train, weight_of_data, w_initial, b_initial, 4.0e-7, 50)\n",
    "\n",
    "print(w_final)\n",
    "print(b_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = x_train[:2000, :]\n",
    "target_dataset = y_train[:2000]\n",
    "\n",
    "# w_test = np.zeros(x_train.shape[1])\n",
    "# b_test = 0\n",
    "\n",
    "prediction = PredictOutput(test_dataset, w_final, b_final)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.1\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(2000):\n",
    "    sum += prediction[i] == target_dataset[i]\n",
    "print(sum / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Using scikit-learn to build our model'''\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "income_model = DecisionTreeClassifier(random_state=1)\n",
    "income_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(income_model.predict(x_train[:30, :]) == y_train[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = features_data[:28000, :13]\n",
    "y_train = np.array(df['Income'] == ' >50K').astype(int)[:28000]\n",
    "\n",
    "income_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ' Female'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m y_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(df[\u001b[39m'\u001b[39m\u001b[39mIncome\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m >50K\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)[\u001b[39m28000\u001b[39m:\u001b[39m29000\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(income_model\u001b[39m.\u001b[39;49mpredict(features_data[\u001b[39m28000\u001b[39;49m:\u001b[39m29000\u001b[39;49m, :\u001b[39m13\u001b[39;49m]) \u001b[39m==\u001b[39m y_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py:426\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[39m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \n\u001b[1;32m    405\u001b[0m \u001b[39mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    425\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 426\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X, check_input)\n\u001b[1;32m    427\u001b[0m proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree_\u001b[39m.\u001b[39mpredict(X)\n\u001b[1;32m    428\u001b[0m n_samples \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py:392\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[39mif\u001b[39;00m check_input:\n\u001b[0;32m--> 392\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49mDTYPE, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    393\u001b[0m     \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (\n\u001b[1;32m    394\u001b[0m         X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc\n\u001b[1;32m    395\u001b[0m     ):\n\u001b[1;32m    396\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    534\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 535\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    536\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    537\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:877\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    875\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    876\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    878\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    879\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    880\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    881\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: ' Female'"
     ]
    }
   ],
   "source": [
    "y_test = np.array(df['Income'] == ' >50K').astype(int)[28000:29000]\n",
    "\n",
    "print(income_model.predict(features_data[28000:29000, :13]) == y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
